{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset Merge IDEA\n\n1. history에 대해 drop duplicate 후 drop column      \n2. watch 에 대해서도 drop duplicate 후 drop column     \n3. 이후 inner join       \n4. 다른 dataset merge   \n\n문제 : 3번부터 램이 터져나감    \n대안 1. 나눠서 조인 (노가다...)   \n대안 2. 더 나은 노트북 환경 찾기   \n대안 3. 똑똑하게 머리 쓰기 **<해결 방안 찾아냄>**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-14T11:51:46.048464Z","iopub.execute_input":"2022-11-14T11:51:46.049402Z","iopub.status.idle":"2022-11-14T11:51:46.089332Z","shell.execute_reply.started":"2022-11-14T11:51:46.049223Z","shell.execute_reply":"2022-11-14T11:51:46.087692Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### 시청 시작 데이터에서 row 중복 체크 - 1만 6천개 가량의 중복 확인. Drop","metadata":{}},{"cell_type":"code","source":"'''\n의심의 여지 없이 중복 존재 \n아래 코드 실행 시 groupby로 상당히 많이 묶이고, 최종 column 개수가 899021로 줄었음을 알 수 있음\n''' \n# history.groupby(['profile_id','log_time','ss_id']).count().sort_values('album_id',ascending=False)[:60]","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:46.091891Z","iopub.execute_input":"2022-11-14T11:51:46.092338Z","iopub.status.idle":"2022-11-14T11:51:46.103719Z","shell.execute_reply.started":"2022-11-14T11:51:46.092305Z","shell.execute_reply":"2022-11-14T11:51:46.102130Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'\\n의심의 여지 없이 중복 존재 \\n아래 코드 실행 시 groupby로 상당히 많이 묶이고, 최종 column 개수가 899021로 줄었음을 알 수 있음\\n'"},"metadata":{}}]},{"cell_type":"code","source":"history = pd.read_csv('../input/lgground/history_data.csv')\nhistory.drop_duplicates(subset=['profile_id','log_time','album_id'],inplace=True) \n# row = 1005651 rows × 8 columns\n# after drop duplicates 899021 rows × 8 columns\n# 106630 개 row 중복","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:46.106090Z","iopub.execute_input":"2022-11-14T11:51:46.107058Z","iopub.status.idle":"2022-11-14T11:51:47.939537Z","shell.execute_reply.started":"2022-11-14T11:51:46.107009Z","shell.execute_reply":"2022-11-14T11:51:47.938188Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 'continuous_play','short_trailer','log_time' -- drop\n# payment 결측 0으로 대체\n# history.drop(labels=['log_time','act_target_dtl','continuous_play','short_trailer'],axis=1).fillna(0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:47.941444Z","iopub.execute_input":"2022-11-14T11:51:47.941885Z","iopub.status.idle":"2022-11-14T11:51:47.948410Z","shell.execute_reply.started":"2022-11-14T11:51:47.941852Z","shell.execute_reply":"2022-11-14T11:51:47.946484Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Watch에 대해서도 동일한 작업 수행 (duplicate drop, feature selection)","metadata":{}},{"cell_type":"code","source":"watch = pd.read_csv('../input/lgground/watch_e_data.csv')\nwatch.drop_duplicates(subset=['profile_id','log_time','album_id'],inplace=True) \n# watch.drop(labels=['log_time','act_target_dtl'],axis=1,inplace=True)  # column 드랍. 따로 처리할 결측치 없음","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:47.952297Z","iopub.execute_input":"2022-11-14T11:51:47.952778Z","iopub.status.idle":"2022-11-14T11:51:49.462347Z","shell.execute_reply.started":"2022-11-14T11:51:47.952744Z","shell.execute_reply":"2022-11-14T11:51:49.461012Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# full 병합 코드는 폭탄 그 자체\n# 돌렸다 하면 터져용 \n# datamerge = pd.merge(history,watch,how='inner',on='profile_id')\n# datamerge.to_csv('../input/lgground')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:49.463881Z","iopub.execute_input":"2022-11-14T11:51:49.464398Z","iopub.status.idle":"2022-11-14T11:51:49.469545Z","shell.execute_reply.started":"2022-11-14T11:51:49.464336Z","shell.execute_reply":"2022-11-14T11:51:49.468347Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## 터진다...! 💣💥\n*근데 여기서 문득 드는 생각*\n\n1. 반드시 history와 watch를 inner join으로 묶어야 할까? 그러니까... serve할 비중복 유저 리스트만 history에서 가져오고, 제대로 된 neg/pos sampling이 가능한 watch 데이터에 붙여서 그것 중심으로 학습할 수도 있지 않나?     \n애초에 history에서 유의미한 column은 **시청 시작 데이터에만 있는 profile_id & 그나마 payment 뿐이다**    \nㄴ *그나마 payment* : payment는 영상 시청의 장벽 같은 요소로 작동하기 때문에 (무료 영상이 유료 영상보다 많이 재생된다) sampling 혹은 추천 요소에 중요하게 고려할 만 하다.    \n.       \n           \n이렇게 접근하면 어떤 방식이 가능해지냐면 < log_time column도 날릴 수 있게 된다 :: 그래도 train-vaild를 위해서 날리진 말자 >    \n1. 두 dataset에서 필요 없는 column을 drop하고       \n2. history에만 있는 profile_id 가 있는 row를 전부 추출해 watch에 append한다.\n3. 추후 데이터셋 분할을 고려해, log_time 기준으로 dataframe 정렬 sort.    \n3. 위 데이터셋에 meta + profile 데이터셋 merge 수행한다.    \n     \n> 최종적으로는 watch에 있는 중요한 값을 기본으로 가지고, serve대상인 유저목록까지 챙긴 dataset이 됨     ","metadata":{}},{"cell_type":"code","source":"# data preprocessing\n# history에만 있는 profile_id  Length: 8311 / watch에만 있는 profile_id  Length: 7658\nm_his = history['profile_id'].drop_duplicates() # 터짐 방지로 profile_id column만 남김 \nm_wat = watch['profile_id'].drop_duplicates()\nid_only_in_history = pd.merge(m_his,m_wat,how='outer',indicator=True\n                ).query('_merge == \"left_only\"').drop(columns=['_merge'])\nid_only_in_history_list = id_only_in_history['profile_id'].to_list()\nid_only_in_history_rows = history[history['profile_id'].isin(id_only_in_history_list)]\nid_only_in_history_rows = id_only_in_history_rows.drop_duplicates(subset=['profile_id','ss_id','log_time','album_id']) # 15241 rows\nid_only_in_history_rows.drop(columns=['continuous_play','short_trailer'])\ndataset = watch.append(id_only_in_history_rows,sort=False).sort_values('log_time').reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:49.470984Z","iopub.execute_input":"2022-11-14T11:51:49.471357Z","iopub.status.idle":"2022-11-14T11:51:50.038656Z","shell.execute_reply.started":"2022-11-14T11:51:49.471324Z","shell.execute_reply":"2022-11-14T11:51:50.037078Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# dataset verification\n# watch : 800632 +15241 = row 815873 나와야 정상병합\nprint(dataset['total_time'].isnull().sum()) #제대로 합쳐졌다면 결측이 15241개 있을 것\nprint(dataset['profile_id'].nunique()) #제대로 합쳐졌다면 8311명일 것 ","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:50.040504Z","iopub.execute_input":"2022-11-14T11:51:50.040877Z","iopub.status.idle":"2022-11-14T11:51:50.057192Z","shell.execute_reply.started":"2022-11-14T11:51:50.040847Z","shell.execute_reply":"2022-11-14T11:51:50.055347Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"15241\n8311\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset.to_csv('./dataset.csv') \ndataset.info()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:50.058914Z","iopub.execute_input":"2022-11-14T11:51:50.059294Z","iopub.status.idle":"2022-11-14T11:51:54.063653Z","shell.execute_reply.started":"2022-11-14T11:51:50.059255Z","shell.execute_reply":"2022-11-14T11:51:54.062430Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 815873 entries, 0 to 815872\nData columns (total 10 columns):\n #   Column           Non-Null Count   Dtype  \n---  ------           --------------   -----  \n 0   profile_id       815873 non-null  int64  \n 1   ss_id            815873 non-null  int64  \n 2   log_time         815873 non-null  int64  \n 3   act_target_dtl   815873 non-null  object \n 4   album_id         815873 non-null  int64  \n 5   watch_time       800632 non-null  float64\n 6   total_time       800632 non-null  float64\n 7   continuous_play  815873 non-null  object \n 8   payment          564 non-null     float64\n 9   short_trailer    15241 non-null   object \ndtypes: float64(3), int64(4), object(3)\nmemory usage: 62.2+ MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Meta Data merge ","metadata":{}},{"cell_type":"code","source":"# # 메타데이터 병합 \nmeta = pd.read_csv('../input/lgground/meta_data.csv') # 42602 rows\nmeta_p = pd.read_csv('../input/lgground/meta_data_plus.csv') # 767948 rows\n# meta_merge = pd.merge(meta,meta_p,how='inner',on='album_id') #### meta 정보 최종 집합체 : 832356 rows\n# data_w_meta = pd.merge(dataset,meta,how='inner',on='album_id')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:54.065095Z","iopub.execute_input":"2022-11-14T11:51:54.065755Z","iopub.status.idle":"2022-11-14T11:51:54.885508Z","shell.execute_reply.started":"2022-11-14T11:51:54.065713Z","shell.execute_reply":"2022-11-14T11:51:54.883980Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3552: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 문제발생.... 😵\n\nmeta 와 meta plus 데이터의 총 row 개수가 많이 차이난다. 그냥 merge해서 해결될 문제는 아님   \n복잡하구마안   \n  \n> 아 맞다... meta_plus는 앨범 하나에 여러 개의 키워드가 할당되어 있었지...    \npreprocessing 해야 한다!     \n추후 모델 학습에 지장이 없을 형태로 바꿔두려면.... 흠....\n>> 유니크한 album 개수는 똑같다\n    \n일단 meta_plus['keyword_name'].nunique()는 1062개. 이거에 대한 인코딩을 진행해서 다시 붙여주면 되지 않을까?     \n+ keyword_type의 nunique도 1062개일까? (나름 인코딩이 된 걸까?) >> 아니다! 1101개다! (ㅎㅎ.. 뭐야 왜있는거람)  \n++ 그나마 다행? : [keyword_name]에 결측 row는 없다. 인코딩만 잘하면 됨 \n\nkeyword value도 엄청 신경쓰인다... 제대로 분별력을 가진 데이터라고 믿어보는수밖에 ㅠ.ㅠ     \n일단은 keyword_name에 feature encoding을 진행한다. 이후 keyword_type은 drop     ","metadata":{}},{"cell_type":"markdown","source":"## Meta_plus 데이터 proprocess ","metadata":{}},{"cell_type":"code","source":"meta_p['keyword_type'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:54.887340Z","iopub.execute_input":"2022-11-14T11:51:54.887920Z","iopub.status.idle":"2022-11-14T11:51:54.955066Z","shell.execute_reply.started":"2022-11-14T11:51:54.887864Z","shell.execute_reply":"2022-11-14T11:51:54.953561Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"1101"},"metadata":{}}]},{"cell_type":"code","source":"# display(meta_p[meta_p['album_id']==390]) \n\n# 1개 키워드 개수는 30개 가량뿐이다. 일단 keyword를 신뢰할 만하다고 판단.\n# 5세와 5세 추천은 합치지 않는 게 낫다. 사유는 아래 코드 실행 \n# 다른 나이를 위해 만든 영상임에도 5세가 볼만하다면 제시되는 키워드. 5세와 5세 추천 키워드가 같이 쓰인 앨범은 없다는 게 조금 신경쓰인다.\n# display(meta_p[(meta_p['keyword_name']=='5세 추천')&(meta_p['keyword_name']=='5세')]) \n# 중복 키워드 결과 없음\n# display(meta_p[(meta_p['keyword_name']=='비교')&(meta_p['keyword_name']=='비교하기')])  \n# display(meta_p[(meta_p['keyword_name']=='야외놀이')&(meta_p['keyword_name']=='야외놀이안전')]) \n# display(meta_p[(meta_p['keyword_name']=='언어1')&(meta_p['keyword_name']=='언어2')]) \n# display(meta_p[(meta_p['keyword_name']=='언어지능1')&(meta_p['keyword_name']=='언어지능2')]) ","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:54.957008Z","iopub.execute_input":"2022-11-14T11:51:54.957504Z","iopub.status.idle":"2022-11-14T11:51:54.964525Z","shell.execute_reply.started":"2022-11-14T11:51:54.957467Z","shell.execute_reply":"2022-11-14T11:51:54.962648Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows',None)\n# meta_p['keyword_name'].value_counts() # 1개 키워드 개수는 30개 가량뿐이다. 일단 keyword를 신뢰할 만하다고 판단.\n\n# 한편, 아주 비슷한 keyword끼리는 encoding 전에 같은 태그로 통일할 필요가 있음. (ex. 치아 구강 ↔ 치아,  용감함 ↔ 용감함4)\n# 이걸 확인하기 위해서, sort로 1062개를 정렬해본다. (휴먼 러닝을 피할 수가 없음)\n\nkey_count = pd.DataFrame(meta_p['keyword_name'].value_counts().reset_index(drop=False).sort_values(by='index'))\n# key_count","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-11-14T11:51:54.966386Z","iopub.execute_input":"2022-11-14T11:51:54.966839Z","iopub.status.idle":"2022-11-14T11:51:55.041310Z","shell.execute_reply.started":"2022-11-14T11:51:54.966798Z","shell.execute_reply":"2022-11-14T11:51:55.039706Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### 확인해 봐야 하는 (합쳐야 하는 단어 목록)\n5세(05010403) > 5세 추천(050b0102)    \n6세 > 6세 추천(050b0103)    \n7세 > 7세 추천    \n비교 < 비교하기    \n안데르센 > 안데르센상    \n야외놀이 > 야외놀이안전 (확인필요)    \n언어1, 언어2 (확인필요)     \n언어지능1, 언어지능2    \n용감함 > 용감함 4    \n의사소통 < 의사소통(Communication)    \n자신감 < 자신감(Confidence)    \n장선혜 > 장선혜(작화)    \n최숙희 > 최숙희(작화)    \n캐리 < 캐리TV\n> 숫자가 더 적은 쪽을 변경하기로 한다. ","metadata":{}},{"cell_type":"code","source":"# 태그 코드는 떨어트릴 거지만 그래도 같이 변경해주자.\n\nmeta_p.replace({'keyword_name' : '안데르센상'}, '안데르센',inplace=True)\nmeta_p.replace({'keyword_name' : '용감함 4'}, '용감함',inplace=True)\nmeta_p.replace({'keyword_name' : '의사소통'}, '의사소통(Communication)',inplace=True)\nmeta_p.replace({'keyword_name' : '자신감'}, '자신감(Confidence)',inplace=True)\nmeta_p.replace({'keyword_name' : '장선혜'}, '장선혜(작화)',inplace=True)\nmeta_p.replace({'keyword_name' : '최숙희'}, '최숙희(작화)',inplace=True)\nmeta_p.replace({'keyword_name' : '캐리'}, '캐리TV',inplace=True)\n\nmeta_p.replace({'keyword_type' : '05010a01'}, '0501090d',inplace=True)\nmeta_p.replace({'keyword_type' : '0509071c'}, '05090708',inplace=True)\nmeta_p.replace({'keyword_type' : '05030908'}, '050c1002',inplace=True)\nmeta_p.replace({'keyword_type' : '05070607'}, '050c1006',inplace=True)\nmeta_p.replace({'keyword_type' : '05010907'}, '05010914',inplace=True)\nmeta_p.replace({'keyword_type' : '05010909'}, '05010915',inplace=True)\nmeta_p.replace({'keyword_type' : '05080409'}, '0501080a',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:55.048063Z","iopub.execute_input":"2022-11-14T11:51:55.048929Z","iopub.status.idle":"2022-11-14T11:51:55.557862Z","shell.execute_reply.started":"2022-11-14T11:51:55.048879Z","shell.execute_reply":"2022-11-14T11:51:55.556608Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"meta_p['keyword_type'].nunique()\n#제대로 정리됨","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:55.559488Z","iopub.execute_input":"2022-11-14T11:51:55.559901Z","iopub.status.idle":"2022-11-14T11:51:55.623739Z","shell.execute_reply.started":"2022-11-14T11:51:55.559865Z","shell.execute_reply":"2022-11-14T11:51:55.622413Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"1094"},"metadata":{}}]},{"cell_type":"markdown","source":"# (meta+plus)data Merge 작업 ","metadata":{}},{"cell_type":"code","source":"# sub_title 리스트 담기\nmeta_df_sub_title_list_dict= {}\n\nfor idx, row in meta.iterrows():\n    if meta_df_sub_title_list_dict.get(row['album_id'], None) == None:\n        meta_df_sub_title_list_dict[row['album_id']] = [row['sub_title']]\n    elif row['sub_title'] not in meta_df_sub_title_list_dict[row['album_id']]:\n        meta_df_sub_title_list_dict[row['album_id']] = meta_df_sub_title_list_dict[row['album_id']]+[row['sub_title']]\n        \nprint('전체 컨텐츠 개수:',len(set(meta.album_id)),'딕셔너리 담긴 아이템 개수:',len(meta_df_sub_title_list_dict))        \n\nmeta_df_prepro_sub_title = meta.copy()\nmeta_df_prepro_sub_title['sub_title'] = meta_df_prepro_sub_title['album_id'].apply(lambda x: meta_df_sub_title_list_dict.get(x, None))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:55.624990Z","iopub.execute_input":"2022-11-14T11:51:55.625323Z","iopub.status.idle":"2022-11-14T11:51:58.532918Z","shell.execute_reply.started":"2022-11-14T11:51:55.625294Z","shell.execute_reply":"2022-11-14T11:51:58.531311Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"전체 컨텐츠 개수: 39875 딕셔너리 담긴 아이템 개수: 39875\n","output_type":"stream"}]},{"cell_type":"code","source":"# 전체 중복 삭제하기\nmeta_df_prepro_sub_title = meta_df_prepro_sub_title.drop_duplicates(subset=['album_id'], ignore_index=True)\n\nprint('전체 데이터수와 album_id 개수 동일 확인:', len(meta_df_prepro_sub_title), len(meta_df_prepro_sub_title.album_id.unique()))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:58.534667Z","iopub.execute_input":"2022-11-14T11:51:58.535074Z","iopub.status.idle":"2022-11-14T11:51:58.568059Z","shell.execute_reply.started":"2022-11-14T11:51:58.535042Z","shell.execute_reply":"2022-11-14T11:51:58.566172Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"전체 데이터수와 album_id 개수 동일 확인: 39875 39875\n","output_type":"stream"}]},{"cell_type":"code","source":"# 메타 플러스 데이터 keyword(keyword_name, type)리스트 담기\nmeta_plus_type_name = meta_p.copy()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:58.570086Z","iopub.execute_input":"2022-11-14T11:51:58.570553Z","iopub.status.idle":"2022-11-14T11:51:58.587924Z","shell.execute_reply.started":"2022-11-14T11:51:58.570517Z","shell.execute_reply":"2022-11-14T11:51:58.586428Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"type_to_name = {}\n\nfor idx, row in meta_p.iterrows():\n    type_to_name[row['keyword_type']] = row['keyword_name']\n    \nmeta_plus_type_name['keyword_name'] = meta_plus_type_name['keyword_type'].apply(lambda x: type_to_name[x])    \n\nname_to_type = {}\nfor idx, row in meta_p.iterrows():\n    name_to_type[row['keyword_name']] = row['keyword_type']\n    \nmeta_plus_type_name['keyword_type'] = meta_plus_type_name['keyword_name'].apply(lambda x: name_to_type[x])\nprint('keyword_name, keyword_type 개수 확인:',len(set(meta_plus_type_name.keyword_name)), len(set(meta_plus_type_name.keyword_type)))\nprint('앨범 당 키워드 중복 여부 확인:',len(meta_p),len(meta_p.drop_duplicates(subset=['album_id','keyword_type'])))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:51:58.589756Z","iopub.execute_input":"2022-11-14T11:51:58.590153Z","iopub.status.idle":"2022-11-14T11:53:26.839636Z","shell.execute_reply.started":"2022-11-14T11:51:58.590119Z","shell.execute_reply":"2022-11-14T11:53:26.838199Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"keyword_name, keyword_type 개수 확인: 1055 1055\n앨범 당 키워드 중복 여부 확인: 767948 766943\n","output_type":"stream"}]},{"cell_type":"code","source":"meta_plus_df_prepro_keyword = meta_plus_type_name.copy()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:53:26.841603Z","iopub.execute_input":"2022-11-14T11:53:26.842456Z","iopub.status.idle":"2022-11-14T11:53:26.857263Z","shell.execute_reply.started":"2022-11-14T11:53:26.842402Z","shell.execute_reply":"2022-11-14T11:53:26.855887Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# meta plus data type name 동일하게 만들기\n\nmeta_df_keyword_name_list_dict= {}\nmeta_df_keyword_type_list_dict= {}\n\nfor idx, row in meta_plus_df_prepro_keyword.iterrows():\n    if meta_df_keyword_name_list_dict.get(row['album_id'], None) == None:\n        meta_df_keyword_name_list_dict[row['album_id']] = [row['keyword_name']]\n    elif row['keyword_name'] not in meta_df_keyword_name_list_dict[row['album_id']]:\n        meta_df_keyword_name_list_dict[row['album_id']] = meta_df_keyword_name_list_dict[row['album_id']]+[row['keyword_name']]\n\nfor idx, row in meta_plus_df_prepro_keyword.iterrows():\n    if meta_df_keyword_type_list_dict.get(row['album_id'], None) == None:\n        meta_df_keyword_type_list_dict[row['album_id']] = [row['keyword_type']]\n    elif row['keyword_type'] not in meta_df_keyword_type_list_dict[row['album_id']]:\n        meta_df_keyword_type_list_dict[row['album_id']] = meta_df_keyword_type_list_dict[row['album_id']]+[row['keyword_type']]        \n\nmeta_plus_df_prepro_keyword['keyword_name'] = meta_plus_df_prepro_keyword['album_id'].apply(lambda x: meta_df_keyword_name_list_dict.get(x, None))\nmeta_plus_df_prepro_keyword['keyword_type'] = meta_plus_df_prepro_keyword['album_id'].apply(lambda x: meta_df_keyword_type_list_dict.get(x, None))\n\nmeta_plus_df_keyword = meta_plus_df_prepro_keyword.drop_duplicates(subset=['album_id'],ignore_index=True).drop(columns=['keyword_value'])","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:53:26.859071Z","iopub.execute_input":"2022-11-14T11:53:26.859617Z","iopub.status.idle":"2022-11-14T11:55:22.502603Z","shell.execute_reply.started":"2022-11-14T11:53:26.859566Z","shell.execute_reply":"2022-11-14T11:55:22.501139Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# meta data merge\nall_meta_df = pd.merge(meta_df_prepro_sub_title, meta_plus_df_keyword, how='left',on='album_id')\nall_meta_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:55:22.504761Z","iopub.execute_input":"2022-11-14T11:55:22.505294Z","iopub.status.idle":"2022-11-14T11:55:22.600496Z","shell.execute_reply.started":"2022-11-14T11:55:22.505246Z","shell.execute_reply":"2022-11-14T11:55:22.599030Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 39875 entries, 0 to 39874\nData columns (total 18 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   album_id      39875 non-null  int64  \n 1   title         39875 non-null  object \n 2   sub_title     39875 non-null  object \n 3   genre_large   39875 non-null  object \n 4   genre_mid     39875 non-null  object \n 5   genre_small   12279 non-null  object \n 6   country       31773 non-null  object \n 7   run_time      39875 non-null  int64  \n 8   onair_date    5208 non-null   float64\n 9   cast_1        26309 non-null  object \n 10  cast_2        21217 non-null  object \n 11  cast_3        15885 non-null  object \n 12  cast_4        11980 non-null  object \n 13  cast_5        6146 non-null   object \n 14  cast_6        2540 non-null   object \n 15  cast_7        743 non-null    object \n 16  keyword_type  39875 non-null  object \n 17  keyword_name  39875 non-null  object \ndtypes: float64(1), int64(2), object(15)\nmemory usage: 5.8+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"all_meta_df.to_csv('./all_meta_df.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:55:22.602493Z","iopub.execute_input":"2022-11-14T11:55:22.603214Z","iopub.status.idle":"2022-11-14T11:55:23.454428Z","shell.execute_reply.started":"2022-11-14T11:55:22.603175Z","shell.execute_reply":"2022-11-14T11:55:23.453453Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# 최종! dataset(history+watch) + all_meta_df(meta+plus) 데이터 merge \n## payment 까지 반영 ","metadata":{}},{"cell_type":"code","source":"data1 = pd.merge(dataset, all_meta_df, how='left',on='album_id')\n# 아래처럼 병합하면 data2 의 column명에 _x, _y같은 값이 붙어있는 문제 발생 \n# data2 = pd.merge(data1,history, how='left', on=['ss_id','album_id'])\n\n\n# payment 정보 더하기  \n'''\nhistory에서 [payment/log_time/album_id]만 가진 dataframe 만들기\ndata1에 left join 하는데, 기준을 album_id, log_time \n== 같은 시점에 유료였던 것은 유료로 값이 들어가겠죠? (payment는 시간을 고려할수밖에 없습니다)\nㄴ (보류) 방금 진수님이 이야기한 걸 바탕으로는 : 유저 개개인도 payment를 따지는 데 중요한 고려 요소인 것 같아요? <추후 확인>\n   유저가 한번 결제한 영상 > \n'''\n\nhistory_pay = history[['payment','ss_id','album_id']].drop_duplicates()\ndata = pd.merge(data1, history_pay, how='left', on=['payment','ss_id','album_id'])","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:56:11.441168Z","iopub.execute_input":"2022-11-14T11:56:11.441615Z","iopub.status.idle":"2022-11-14T11:56:13.657080Z","shell.execute_reply.started":"2022-11-14T11:56:11.441579Z","shell.execute_reply":"2022-11-14T11:56:13.655341Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()\n# log time, album on으로 merge : 815309\n# ss_id, album on으로 merge :  815309\n\n# 둘 다 똑같네 . . . ","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:56:13.659702Z","iopub.execute_input":"2022-11-14T11:56:13.660120Z","iopub.status.idle":"2022-11-14T11:56:14.321607Z","shell.execute_reply.started":"2022-11-14T11:56:13.660086Z","shell.execute_reply":"2022-11-14T11:56:14.320111Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"profile_id              0\nss_id                   0\nlog_time                0\nact_target_dtl          0\nalbum_id                0\nwatch_time          15241\ntotal_time          15241\ncontinuous_play         0\npayment            815309\nshort_trailer      800632\ntitle                   0\nsub_title               0\ngenre_large             0\ngenre_mid               0\ngenre_small        689896\ncountry            188209\nrun_time                0\nonair_date         799993\ncast_1             322574\ncast_2             418002\ncast_3             481612\ncast_4             548440\ncast_5             699079\ncast_6             793844\ncast_7             807826\nkeyword_type            0\nkeyword_name            0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"data.to_csv('./data.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:55:26.619865Z","iopub.execute_input":"2022-11-14T11:55:26.620393Z","iopub.status.idle":"2022-11-14T11:55:49.719362Z","shell.execute_reply.started":"2022-11-14T11:55:26.620338Z","shell.execute_reply":"2022-11-14T11:55:49.717507Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# pay_his = history[['profile_id','album_id','ss_id']].drop_duplicates()\n# dataset = pd.merge(dataset,pay_his,how='inner',on=['profile_id','album_id','ss_id'])","metadata":{"execution":{"iopub.status.busy":"2022-11-14T11:55:49.721708Z","iopub.execute_input":"2022-11-14T11:55:49.722332Z","iopub.status.idle":"2022-11-14T11:55:49.730201Z","shell.execute_reply.started":"2022-11-14T11:55:49.722270Z","shell.execute_reply":"2022-11-14T11:55:49.728439Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## 잊지 말아야 할 것\n> history의 nunique한 아이템 수 : 대략 2만건   \nmata의 nunique한 아이템 수 : 대략 4만건   \n== 2달간의 소비데이터에 등장조차 하지 않은 아이템이 2만개나 있다\n\n> **todo** 한 달씩 분할해서 / 첫 달에 등장한 아이템이 두번째 달에 많이 등장하는지 확인해보면 될 것 ","metadata":{}},{"cell_type":"markdown","source":"# feature encoding\n\n\n. . . loading","metadata":{}},{"cell_type":"markdown","source":"# Neg Samplig\n\n시청하지 않은 것에 대해서 [시청 시간 feature]를 넣어줘야 함     \nㄴ 시청하지 않은 영상이 너무 많아서 네거티브 샘플링을 하면 편중/과적합이 된다     \nㄴ 관련 논문이나 연구가 있을 법 한데 (찾아보기)\n\n\n진수's idea\n어차피 랜덤하게 샘플링하니까 시청 시간도 랜덤하게 값을 넣어보자 (ex. 해당 아이템의 평균 시청시간)\n*세션이 해당 유저가 들어온 시간이라면, 그 시간에 대해서는 로그*\n유저가 연속해서 재생한 것과, 종료한 다음 재생한 것에 대해 기록에 차이를 줘야 하지 않나?","metadata":{}}]}