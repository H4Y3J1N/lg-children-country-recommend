{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset Merge IDEA\n\n1. history에 대해 drop duplicate 후 drop column      \n2. watch 에 대해서도 drop duplicate 후 drop column     \n3. 이후 inner join       \n4. 다른 dataset merge   \n\n문제 : 3번부터 램이 터져나감    \n대안 1. 나눠서 조인 (노가다...)   \n대안 2. 더 나은 노트북 환경 찾기   \n대안 3. 똑똑하게 머리 쓰기 **<해결 방안 찾아냄>**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-12T02:20:12.209304Z","iopub.execute_input":"2022-11-12T02:20:12.210031Z","iopub.status.idle":"2022-11-12T02:20:12.220656Z","shell.execute_reply.started":"2022-11-12T02:20:12.209966Z","shell.execute_reply":"2022-11-12T02:20:12.219263Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### 시청 시작 데이터에서 row 중복 체크 - 1만 6천개 가량의 중복 확인. Drop","metadata":{}},{"cell_type":"code","source":"'''\n의심의 여지 없이 중복 존재 \n아래 코드 실행 시 groupby로 상당히 많이 묶이고, 최종 column 개수가 899021로 줄었음을 알 수 있음\n''' \n# history.groupby(['profile_id','log_time','ss_id']).count().sort_values('album_id',ascending=False)[:60]","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:20:12.227326Z","iopub.execute_input":"2022-11-12T02:20:12.227696Z","iopub.status.idle":"2022-11-12T02:20:12.237643Z","shell.execute_reply.started":"2022-11-12T02:20:12.227653Z","shell.execute_reply":"2022-11-12T02:20:12.236696Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'\\n의심의 여지 없이 중복 존재 \\n아래 코드 실행 시 groupby로 상당히 많이 묶이고, 최종 column 개수가 899021로 줄었음을 알 수 있음\\n'"},"metadata":{}}]},{"cell_type":"code","source":"history = pd.read_csv('../input/lgground/history_data.csv')\nhistory.drop_duplicates(subset=['profile_id','log_time','album_id'],inplace=True) \n# row = 1005651 rows × 8 columns\n# after drop duplicates 899021 rows × 8 columns\n# 106630 개 row 중복","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:20:12.238850Z","iopub.execute_input":"2022-11-12T02:20:12.239561Z","iopub.status.idle":"2022-11-12T02:20:13.236332Z","shell.execute_reply.started":"2022-11-12T02:20:12.239518Z","shell.execute_reply":"2022-11-12T02:20:13.235138Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 'continuous_play','short_trailer','log_time' -- drop\n# payment 결측 0으로 대체\n# history.drop(labels=['log_time','act_target_dtl','continuous_play','short_trailer'],axis=1).fillna(0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:20:13.240348Z","iopub.execute_input":"2022-11-12T02:20:13.241053Z","iopub.status.idle":"2022-11-12T02:20:13.245508Z","shell.execute_reply.started":"2022-11-12T02:20:13.241016Z","shell.execute_reply":"2022-11-12T02:20:13.244383Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Watch에 대해서도 동일한 작업 수행 (duplicate drop, feature selection)","metadata":{}},{"cell_type":"code","source":"watch = pd.read_csv('../input/lgground/watch_e_data.csv')\nwatch.drop_duplicates(subset=['profile_id','log_time','album_id'],inplace=True) \n# watch.drop(labels=['log_time','act_target_dtl'],axis=1,inplace=True)  # column 드랍. 따로 처리할 결측치 없음","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:20:13.246706Z","iopub.execute_input":"2022-11-12T02:20:13.247043Z","iopub.status.idle":"2022-11-12T02:20:14.023413Z","shell.execute_reply.started":"2022-11-12T02:20:13.247012Z","shell.execute_reply":"2022-11-12T02:20:14.022395Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# full 병합 코드는 폭탄 그 자체\n# 돌렸다 하면 터져용 \n# datamerge = pd.merge(history,watch,how='inner',on='profile_id')\n# datamerge.to_csv('../input/lgground')","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:20:14.024596Z","iopub.execute_input":"2022-11-12T02:20:14.024900Z","iopub.status.idle":"2022-11-12T02:20:14.030136Z","shell.execute_reply.started":"2022-11-12T02:20:14.024870Z","shell.execute_reply":"2022-11-12T02:20:14.029137Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## 터진다...! 💣💥\n*근데 여기서 문득 드는 생각*\n\n1. 반드시 history와 watch를 inner join으로 묶어야 할까? 그러니까... serve할 비중복 유저 리스트만 history에서 가져오고, 제대로 된 neg/pos sampling이 가능한 watch 데이터에 붙여서 그것 중심으로 학습할 수도 있지 않나?     \n애초에 history에서 유의미한 column은 **시청 시작 데이터에만 있는 profile_id & 그나마 payment 뿐이다**    \nㄴ *그나마 payment* : payment는 영상 시청의 장벽 같은 요소로 작동하기 때문에 (무료 영상이 유료 영상보다 많이 재생된다) sampling 혹은 추천 요소에 중요하게 고려할 만 하다.    \n.       \n           \n이렇게 접근하면 어떤 방식이 가능해지냐면 < log_time column도 날릴 수 있게 된다 :: 그래도 train-vaild를 위해서 날리진 말자 >    \n1. 두 dataset에서 필요 없는 column을 drop하고       \n2. history에만 있는 profile_id 가 있는 row를 전부 추출해 watch에 append한다.\n3. 추후 데이터셋 분할을 고려해, log_time 기준으로 dataframe 정렬 sort.    \n3. 위 데이터셋에 meta + profile 데이터셋 merge 수행한다.    \n     \n> 최종적으로는 watch에 있는 중요한 값을 기본으로 가지고, serve대상인 유저목록까지 챙긴 dataset이 됨     ","metadata":{}},{"cell_type":"markdown","source":"# LOG time 으로 duplicate 수정 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!","metadata":{}},{"cell_type":"code","source":"# data preprocessing\n# history에만 있는 profile_id  Length: 8311 / watch에만 있는 profile_id  Length: 7658\nm_his = history['profile_id'].drop_duplicates() # 터짐 방지로 profile_id column만 남김 \nm_wat = watch['profile_id'].drop_duplicates()\nid_only_in_history = pd.merge(m_his,m_wat,how='outer',indicator=True\n                ).query('_merge == \"left_only\"').drop(columns=['_merge'])\nid_only_in_history_list = id_only_in_history['profile_id'].to_list()\nid_only_in_history_rows = history[history['profile_id'].isin(id_only_in_history_list)]\nid_only_in_history_rows = id_only_in_history_rows.drop_duplicates(subset=['profile_id','ss_id','log_time','album_id']) # 15241 rows\nid_only_in_history_rows.drop(columns=['continuous_play','short_trailer'])\ndataset = watch.append(id_only_in_history_rows,sort=False).sort_values('log_time').reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:20:14.031251Z","iopub.execute_input":"2022-11-12T02:20:14.031553Z","iopub.status.idle":"2022-11-12T02:20:14.523098Z","shell.execute_reply.started":"2022-11-12T02:20:14.031525Z","shell.execute_reply":"2022-11-12T02:20:14.522021Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# dataset verification\n# watch : 800632 +15241 = row 815873 나와야 정상병합\nprint(dataset['total_time'].isnull().sum()) #제대로 합쳐졌다면 결측이 15241개 있을 것\nprint(dataset['profile_id'].nunique()) #제대로 합쳐졌다면 8311명일 것 ","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:20:14.524710Z","iopub.execute_input":"2022-11-12T02:20:14.525113Z","iopub.status.idle":"2022-11-12T02:20:14.541700Z","shell.execute_reply.started":"2022-11-12T02:20:14.525070Z","shell.execute_reply":"2022-11-12T02:20:14.540544Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"15241\n8311\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset.info()","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:28:44.936095Z","iopub.execute_input":"2022-11-12T02:28:44.936875Z","iopub.status.idle":"2022-11-12T02:28:45.057008Z","shell.execute_reply.started":"2022-11-12T02:28:44.936839Z","shell.execute_reply":"2022-11-12T02:28:45.055814Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 815873 entries, 0 to 815872\nData columns (total 10 columns):\n #   Column           Non-Null Count   Dtype  \n---  ------           --------------   -----  \n 0   profile_id       815873 non-null  int64  \n 1   ss_id            815873 non-null  int64  \n 2   log_time         815873 non-null  int64  \n 3   act_target_dtl   815873 non-null  object \n 4   album_id         815873 non-null  int64  \n 5   watch_time       800632 non-null  float64\n 6   total_time       800632 non-null  float64\n 7   continuous_play  815873 non-null  object \n 8   payment          564 non-null     float64\n 9   short_trailer    15241 non-null   object \ndtypes: float64(3), int64(4), object(3)\nmemory usage: 62.2+ MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Meta Data + Profile Data merge ","metadata":{}},{"cell_type":"code","source":"# # 메타데이터 병합 \nmetadata = pd.read_csv('../input/lgground/meta_data.csv')\nmeta_p = pd.read_csv('../input/lgground/meta_data_plus.csv')\nmeta = pd.merge(metadata,meta_p,how='inner',on='album_id') #### meta 정보 최종 집합체 \ndata_m = pd.merge(dataset,meta,how='inner',on='album_id')","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:22:59.317649Z","iopub.execute_input":"2022-11-12T02:22:59.318709Z","iopub.status.idle":"2022-11-12T02:23:28.342382Z","shell.execute_reply.started":"2022-11-12T02:22:59.318664Z","shell.execute_reply":"2022-11-12T02:23:28.341418Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3552: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n","output_type":"stream"}]},{"cell_type":"code","source":"data_m.info() # 23743257 rows 가 대체 어케 나오는거임?????????? \n\n# metadata  42602\n# meta_p 767948 entries\n# meta 832356 rows\n\n# 병합 후 data_m  2374325 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ뭔데 뭐냐구 ","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:32:09.755782Z","iopub.execute_input":"2022-11-12T02:32:09.756272Z","iopub.status.idle":"2022-11-12T02:32:09.771414Z","shell.execute_reply.started":"2022-11-12T02:32:09.756226Z","shell.execute_reply":"2022-11-12T02:32:09.770056Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 23743257 entries, 0 to 23743256\nData columns (total 28 columns):\n #   Column           Dtype  \n---  ------           -----  \n 0   profile_id       int64  \n 1   ss_id            int64  \n 2   log_time         int64  \n 3   act_target_dtl   object \n 4   album_id         int64  \n 5   watch_time       float64\n 6   total_time       float64\n 7   continuous_play  object \n 8   payment          float64\n 9   short_trailer    object \n 10  title            object \n 11  sub_title        object \n 12  genre_large      object \n 13  genre_mid        object \n 14  genre_small      object \n 15  country          object \n 16  run_time         int64  \n 17  onair_date       float64\n 18  cast_1           object \n 19  cast_2           object \n 20  cast_3           object \n 21  cast_4           object \n 22  cast_5           object \n 23  cast_6           object \n 24  cast_7           object \n 25  keyword_type     object \n 26  keyword_name     object \n 27  keyword_value    int64  \ndtypes: float64(4), int64(6), object(18)\nmemory usage: 5.1+ GB\n","output_type":"stream"}]},{"cell_type":"code","source":" ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## payments는 따로 history에서 row를 떼어와서 watch로 붙이던가 해야 함 (의미 있는 정보값) \n\n> 어제는 헷갈렸지만, 필요한 작업이다.     \n이 작업을 해야 하는 이유는... 현재 payment가 추출되어 붙여진 rows들에만 붙여진 정보값이기 때문이다. watch에 처음부터 있었던 rows들에는 싹 결측으로 되어 있으므로, album_id + log_time + profile_id 기준으로 join한다면 당시의 payment 가격만 추가할 수 있다 . \n","metadata":{}},{"cell_type":"code","source":"# pay_his = history[['profile_id','album_id','ss_id']].drop_duplicates()\n# dataset = pd.merge(dataset,pay_his,how='inner',on=['profile_id','album_id','ss_id'])","metadata":{"execution":{"iopub.status.busy":"2022-11-12T02:20:16.037348Z","iopub.execute_input":"2022-11-12T02:20:16.037880Z","iopub.status.idle":"2022-11-12T02:20:16.042263Z","shell.execute_reply.started":"2022-11-12T02:20:16.037835Z","shell.execute_reply":"2022-11-12T02:20:16.041248Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## 잊지 말아야 할 것\n> history의 nunique한 아이템 수 : 대략 2만건   \nmata의 nunique한 아이템 수 : 대략 4만건   \n== 2달간의 소비데이터에 등장조차 하지 않은 아이템이 2만개나 있다","metadata":{}},{"cell_type":"markdown","source":"# feature encoding\n\n\nprofile_data.csv 의 경우, 성별 column 수치화 작업 필요  (근데 EDA 가볍게 하고 나서 해도 늦지 않음)     \n    \n사실 feature encoding 작업이 모델학습 전, 전반적으로 필요","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}