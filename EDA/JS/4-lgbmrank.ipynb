{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Merge IDEA\n",
    "\n",
    "1. history에 대해 drop duplicate 후 drop column      \n",
    "2. watch 에 대해서도 drop duplicate 후 drop column     \n",
    "3. 이후 inner join       \n",
    "4. 다른 dataset merge   \n",
    "\n",
    "문제 : 3번부터 램이 터져나감    \n",
    "대안 1. 나눠서 조인 (노가다...)   \n",
    "대안 2. 더 나은 노트북 환경 찾기   \n",
    "대안 3. 똑똑하게 머리 쓰기 **<해결 방안 찾아냄>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-12T02:47:19.700466Z",
     "iopub.status.busy": "2022-11-12T02:47:19.699956Z",
     "iopub.status.idle": "2022-11-12T02:47:19.707583Z",
     "shell.execute_reply": "2022-11-12T02:47:19.705618Z",
     "shell.execute_reply.started": "2022-11-12T02:47:19.700424Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시청 시작 데이터에서 row 중복 체크 - 1만 6천개 가량의 중복 확인. Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:12.227696Z",
     "iopub.status.busy": "2022-11-12T02:20:12.227326Z",
     "iopub.status.idle": "2022-11-12T02:20:12.237643Z",
     "shell.execute_reply": "2022-11-12T02:20:12.236696Z",
     "shell.execute_reply.started": "2022-11-12T02:20:12.227653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n의심의 여지 없이 중복 존재 \\n아래 코드 실행 시 groupby로 상당히 많이 묶이고, 최종 column 개수가 899021로 줄었음을 알 수 있음\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "의심의 여지 없이 중복 존재 \n",
    "아래 코드 실행 시 groupby로 상당히 많이 묶이고, 최종 column 개수가 899021로 줄었음을 알 수 있음\n",
    "''' \n",
    "# history.groupby(['profile_id','log_time','ss_id']).count().sort_values('album_id',ascending=False)[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:12.239561Z",
     "iopub.status.busy": "2022-11-12T02:20:12.238850Z",
     "iopub.status.idle": "2022-11-12T02:20:13.236332Z",
     "shell.execute_reply": "2022-11-12T02:20:13.235138Z",
     "shell.execute_reply.started": "2022-11-12T02:20:12.239518Z"
    }
   },
   "outputs": [],
   "source": [
    "history = pd.read_csv('data/history_data.csv')\n",
    "history.drop_duplicates(subset=['profile_id','log_time','album_id'],inplace=True) \n",
    "# row = 1005651 rows × 8 columns\n",
    "# after drop duplicates 899021 rows × 8 columns\n",
    "# 106630 개 row 중복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:13.241053Z",
     "iopub.status.busy": "2022-11-12T02:20:13.240348Z",
     "iopub.status.idle": "2022-11-12T02:20:13.245508Z",
     "shell.execute_reply": "2022-11-12T02:20:13.244383Z",
     "shell.execute_reply.started": "2022-11-12T02:20:13.241016Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'continuous_play','short_trailer','log_time' -- drop\n",
    "# payment 결측 0으로 대체\n",
    "# history.drop(labels=['log_time','act_target_dtl','continuous_play','short_trailer'],axis=1).fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch에 대해서도 동일한 작업 수행 (duplicate drop, feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:13.247043Z",
     "iopub.status.busy": "2022-11-12T02:20:13.246706Z",
     "iopub.status.idle": "2022-11-12T02:20:14.023413Z",
     "shell.execute_reply": "2022-11-12T02:20:14.022395Z",
     "shell.execute_reply.started": "2022-11-12T02:20:13.247012Z"
    }
   },
   "outputs": [],
   "source": [
    "watch = pd.read_csv('data/watch_e_data.csv')\n",
    "watch.drop_duplicates(subset=['profile_id','log_time','album_id'],inplace=True) \n",
    "# watch.drop(labels=['log_time','act_target_dtl'],axis=1,inplace=True)  # column 드랍. 따로 처리할 결측치 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:14.024900Z",
     "iopub.status.busy": "2022-11-12T02:20:14.024596Z",
     "iopub.status.idle": "2022-11-12T02:20:14.030136Z",
     "shell.execute_reply": "2022-11-12T02:20:14.029137Z",
     "shell.execute_reply.started": "2022-11-12T02:20:14.024870Z"
    }
   },
   "outputs": [],
   "source": [
    "# full 병합 코드는 폭탄 그 자체\n",
    "# 돌렸다 하면 터져용 \n",
    "# datamerge = pd.merge(history,watch,how='inner',on='profile_id')\n",
    "# datamerge.to_csv('../input/lgground')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 터진다...! 💣💥\n",
    "*근데 여기서 문득 드는 생각*\n",
    "\n",
    "1. 반드시 history와 watch를 inner join으로 묶어야 할까? 그러니까... serve할 비중복 유저 리스트만 history에서 가져오고, 제대로 된 neg/pos sampling이 가능한 watch 데이터에 붙여서 그것 중심으로 학습할 수도 있지 않나?     \n",
    "애초에 history에서 유의미한 column은 **시청 시작 데이터에만 있는 profile_id & 그나마 payment 뿐이다**    \n",
    "ㄴ *그나마 payment* : payment는 영상 시청의 장벽 같은 요소로 작동하기 때문에 (무료 영상이 유료 영상보다 많이 재생된다) sampling 혹은 추천 요소에 중요하게 고려할 만 하다.    \n",
    ".       \n",
    "           \n",
    "이렇게 접근하면 어떤 방식이 가능해지냐면 < log_time column도 날릴 수 있게 된다 :: 그래도 train-vaild를 위해서 날리진 말자 >    \n",
    "1. 두 dataset에서 필요 없는 column을 drop하고       \n",
    "2. history에만 있는 profile_id 가 있는 row를 전부 추출해 watch에 append한다.\n",
    "3. 추후 데이터셋 분할을 고려해, log_time 기준으로 dataframe 정렬 sort.    \n",
    "3. 위 데이터셋에 meta + profile 데이터셋 merge 수행한다.    \n",
    "     \n",
    "> 최종적으로는 watch에 있는 중요한 값을 기본으로 가지고, serve대상인 유저목록까지 챙긴 dataset이 됨     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:14.031553Z",
     "iopub.status.busy": "2022-11-12T02:20:14.031251Z",
     "iopub.status.idle": "2022-11-12T02:20:14.523098Z",
     "shell.execute_reply": "2022-11-12T02:20:14.522021Z",
     "shell.execute_reply.started": "2022-11-12T02:20:14.031525Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/2dv1p69j3pv2c66bj9jt_hbc0000gn/T/ipykernel_61340/2297796919.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = watch.append(id_only_in_history_rows,sort=False).sort_values('log_time').reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "# history에만 있는 profile_id  Length: 8311 / watch에만 있는 profile_id  Length: 7658\n",
    "m_his = history['profile_id'].drop_duplicates() # 터짐 방지로 profile_id column만 남김 \n",
    "m_wat = watch['profile_id'].drop_duplicates()\n",
    "id_only_in_history = pd.merge(m_his,m_wat,how='outer',indicator=True\n",
    "                ).query('_merge == \"left_only\"').drop(columns=['_merge'])\n",
    "id_only_in_history_list = id_only_in_history['profile_id'].to_list()\n",
    "id_only_in_history_rows = history[history['profile_id'].isin(id_only_in_history_list)]\n",
    "id_only_in_history_rows = id_only_in_history_rows.drop_duplicates(subset=['profile_id','ss_id','log_time','album_id']) # 15241 rows\n",
    "id_only_in_history_rows.drop(columns=['continuous_play','short_trailer'])\n",
    "dataset = watch.append(id_only_in_history_rows,sort=False).sort_values('log_time').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:20:14.525113Z",
     "iopub.status.busy": "2022-11-12T02:20:14.524710Z",
     "iopub.status.idle": "2022-11-12T02:20:14.541700Z",
     "shell.execute_reply": "2022-11-12T02:20:14.540544Z",
     "shell.execute_reply.started": "2022-11-12T02:20:14.525070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15241\n",
      "8311\n"
     ]
    }
   ],
   "source": [
    "# dataset verification\n",
    "# watch : 800632 +15241 = row 815873 나와야 정상병합\n",
    "print(dataset['total_time'].isnull().sum()) #제대로 합쳐졌다면 결측이 15241개 있을 것\n",
    "print(dataset['profile_id'].nunique()) #제대로 합쳐졌다면 8311명일 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:28:44.936875Z",
     "iopub.status.busy": "2022-11-12T02:28:44.936095Z",
     "iopub.status.idle": "2022-11-12T02:28:45.057008Z",
     "shell.execute_reply": "2022-11-12T02:28:45.055814Z",
     "shell.execute_reply.started": "2022-11-12T02:28:44.936839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 815873 entries, 0 to 815872\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   profile_id       815873 non-null  int64  \n",
      " 1   ss_id            815873 non-null  int64  \n",
      " 2   log_time         815873 non-null  int64  \n",
      " 3   act_target_dtl   815873 non-null  object \n",
      " 4   album_id         815873 non-null  int64  \n",
      " 5   watch_time       800632 non-null  float64\n",
      " 6   total_time       800632 non-null  float64\n",
      " 7   continuous_play  815873 non-null  object \n",
      " 8   payment          564 non-null     float64\n",
      " 9   short_trailer    15241 non-null   object \n",
      "dtypes: float64(3), int64(4), object(3)\n",
      "memory usage: 62.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Data merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-12T02:47:27.172413Z",
     "iopub.status.busy": "2022-11-12T02:47:27.171998Z",
     "iopub.status.idle": "2022-11-12T02:47:28.120125Z",
     "shell.execute_reply": "2022-11-12T02:47:28.118766Z",
     "shell.execute_reply.started": "2022-11-12T02:47:27.172379Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/2dv1p69j3pv2c66bj9jt_hbc0000gn/T/ipykernel_61340/3753341068.py:2: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  meta = pd.read_csv('data/meta_data.csv') # 42602 rows\n"
     ]
    }
   ],
   "source": [
    "# # 메타데이터 병합 \n",
    "meta = pd.read_csv('data/meta_data.csv') # 42602 rows\n",
    "meta_p = pd.read_csv('data/meta_data_plus.csv') # 767948 rows\n",
    "# meta_merge = pd.merge(meta,meta_p,how='inner',on='album_id') #### meta 정보 최종 집합체 : 832356 rows\n",
    "# data_w_meta = pd.merge(dataset,meta,how='inner',on='album_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 \n",
    "class cfg: \n",
    "    gpu_idx = 0\n",
    "    top_k = 25\n",
    "    seed = 42\n",
    "    neg_ratio = 100\n",
    "    test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/2dv1p69j3pv2c66bj9jt_hbc0000gn/T/ipykernel_61340/2801991363.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['rating'] = 1\n"
     ]
    }
   ],
   "source": [
    "data = dataset[['profile_id','album_id']]\n",
    "data['rating'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기: (652698, 3)\n",
      "검증 데이터 크기: (163175, 3)\n"
     ]
    }
   ],
   "source": [
    "cfg.n_users = data.profile_id.max()+1\n",
    "cfg.n_items = data.album_id.max()+1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "from tqdm.notebook import tqdm\n",
    "# 학습 및 검증 데이터 분리\n",
    "train, valid = train_test_split(\n",
    "    data, test_size=cfg.test_size, random_state=cfg.seed,\n",
    ")\n",
    "print('학습 데이터 크기:', train.shape)\n",
    "print('검증 데이터 크기:', valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009532928466796875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 652698,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdebe3c36884aff8552bfcfe46098af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/652698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 형태: \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix 형태로 변환 \n",
    "train = train.to_numpy()\n",
    "matrix = sparse.lil_matrix((cfg.n_users, cfg.n_items))\n",
    "for (p, i, r) in tqdm(train):\n",
    "    matrix[p, i] = r\n",
    "    \n",
    "train = sparse.csr_matrix(matrix)\n",
    "train = train.toarray()\n",
    "print(\"train 형태: \\n\", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_UIdataset(train, neg_ratio):\n",
    "    \"\"\" 유저별 학습에 필요한 딕셔너리 데이터 생성 \n",
    "    Args:\n",
    "        train : 유저-아이템의 상호작용을 담은 행렬 \n",
    "            ex) \n",
    "                array([[0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        ...,\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.]])\n",
    "        neg_ratio : negative sampling 활용할 비율 \n",
    "            ex) 3 (positive label 1개당 negative label 3개)\n",
    "    Returns: \n",
    "        UIdataset : 유저별 학습에 필요한 정보를 담은 딕셔너리 \n",
    "            ex) {'사용자 ID': [[positive 샘플, negative 샘플], ... , [1, 1, 1, ..., 0, 0]]}\n",
    "                >>> UIdataset[3]\n",
    "                    [array([   16,    17,    18, ...,  9586, 18991,  9442]),\n",
    "                    array([5, 5, 5, ..., 5, 5, 5]),\n",
    "                    array([4, 4, 4, ..., 5, 1, 1]),\n",
    "                    array([1., 1., 1., ..., 0., 0., 0.])]\n",
    "    \"\"\"\n",
    "    UIdataset = {'profile_id': [], 'album_id':[],'rating':[]}\n",
    "    for user_id, items_by_user in enumerate(train):\n",
    "#         UIdataset[user_id] = []\n",
    "        # positive 샘플 계산 \n",
    "        pos_item_ids = np.where(items_by_user > 0.5)[0]\n",
    "        num_pos_samples = len(pos_item_ids)\n",
    "\n",
    "        # negative 샘플 계산 (random negative sampling) \n",
    "        num_neg_samples = neg_ratio * num_pos_samples\n",
    "\n",
    "        neg_items = np.where(items_by_user < 0.5)[0]\n",
    "        neg_item_ids = np.random.choice(neg_items, min(num_neg_samples, len(neg_items)), replace=False)\n",
    "        UIdataset['album_id'] += list(neg_item_ids)+list(pos_item_ids)\n",
    "        UIdataset['profile_id'] += [user_id for _ in range(len(neg_item_ids)+len(pos_item_ids))]\n",
    "\n",
    "#         for column in columns:\n",
    "#             # feature 추출 \n",
    "#             features = []\n",
    "#             for item_id in np.concatenate([pos_item_ids, neg_item_ids]): \n",
    "#                 features.append(user_features[column][user_id])\n",
    "#             UIdataset[column].append(np.array(features))\n",
    "\n",
    "#             features = []\n",
    "#             for item_id in np.concatenate([pos_item_ids, neg_item_ids]): \n",
    "#                 features.append(item_features[column][item_id])\n",
    "#             UIdataset[column].append(np.array(features))\n",
    "\n",
    "            # label 저장  \n",
    "        pos_labels = np.ones(len(pos_item_ids))\n",
    "        neg_labels = np.zeros(len(neg_item_ids))\n",
    "        UIdataset['rating'] += list(neg_labels)+list(pos_labels)\n",
    "\n",
    "    return UIdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_neg_samplings = make_UIdataset(train,100)\n",
    "pos_neg_samplings_df = pd.DataFrame(pos_neg_samplings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta[['album_id','title','genre_mid','run_time']].drop_duplicates(subset=['album_id'], ignore_index=True)\n",
    "profile = pd.read_csv('data/profile_data.csv')\n",
    "profile_df = profile[['profile_id','sex','age']]\n",
    "\n",
    "dataset_meta_merge = pd.merge(pos_neg_samplings_df, meta_df, how='left', on='album_id')\n",
    "train_df = pd.merge(dataset_meta_merge,profile_df, how='left', on='profile_id')\n",
    "qids_train = train_df.groupby('profile_id')['profile_id'].count().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_dataset_meta_merge = pd.merge(valid, meta_df, how='left', on='album_id')\n",
    "valid_df = pd.merge(valid_dataset_meta_merge,profile_df, how='left', on='profile_id')\n",
    "qids_validation = valid_df.groupby('profile_id')['profile_id'].count().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df, columns=['album_id','title','genre_mid','sex'],sparse=True)\n",
    "valid_df = pd.get_dummies(valid_df, columns=['album_id','title','genre_mid','sex'],sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightgbm rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "\n",
    "\n",
    "X_train = train_df.drop(columns=['rating'])\n",
    "y_train = train_df['rating']\n",
    "\n",
    "X_validation = valid_df.drop(columns=['rating'])\n",
    "y_validation = valid_df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lightgbm.LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    group=qids_train,\n",
    "    eval_set=[(X_validation, y_validation)],\n",
    "    eval_group=[qids_validation],\n",
    "    eval_at=10,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
